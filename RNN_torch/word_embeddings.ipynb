{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trec/anaconda3/envs/PyBulletRL/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5e985e5f90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix = {'hello':0 , 'world':1}\n",
    "embeds = nn.Embedding(2,5)#2 words in vocab, 5 dimensional embeddings\n",
    "lookup_tensor = torch.tensor([word_to_ix['hello']],dtype=torch.long)\n",
    "print(lookup_tensor)\n",
    "\n",
    "hello_embed = embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Example: N-Gram Language Modeling\n",
    "Recall that in an n-gram language model, given a sequence of words \n",
    "https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "â€‹\n",
    "  is the ith word of the sequence.\n",
    "\n",
    "In this example, we will compute the loss function on some training examples and update the parameters with backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['forty', 'When'], 'winters'), (['winters', 'forty'], 'shall'), (['shall', 'winters'], 'besiege')]\n",
      "{'sum', 'an', 'succession', 'my', 'sunken', 'all-eating', 'so', 'thy', \"totter'd\", 'brow,', 'say,', 'in', 'deep', 'answer', 'much', 'To', 'use,', 'asked,', 'and', 'within', 'dig', 'praise', 'warm', 'forty', 'fair', 'see', \"youth's\", 'besiege', 'Shall', 'count,', 'Proving', 'thine', 'Then', \"deserv'd\", 'his', 'shall', 'days;', 'praise.', \"'This\", \"beauty's\", 'Thy', 'old,', 'winters', 'held:', 'the', 'worth', 'being', 'gazed', 'couldst', 'make', 'it', 'made', 'When', 'where', 'This', 'all', 'of', 'cold.', 'If', \"feel'st\", 'How', 'to', 'shame,', 'now,', 'Where', 'were', 'new', 'lusty', 'small', 'thine!', 'on', 'proud', 'Will', 'by', 'old', 'child', 'weed', 'a', 'when', 'field,', 'own', 'And', 'beauty', 'eyes,', 'more', 'Were', 'thriftless', 'trenches', 'thou', 'be', \"excuse,'\", 'mine', 'art', 'blood', 'treasure', 'livery', 'lies,'}\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "# We will use Shakespeare Sonnet 2\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "# we should tokenize the input, but we will ignore that for now\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "\n",
    "ngrams = [([test_sentence[i-j-1] for j in range(CONTEXT_SIZE)], test_sentence[i]) for i in range(CONTEXT_SIZE, len(test_sentence))]\n",
    "\n",
    "#print the first three to see how they look like\n",
    "print(ngrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "print(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[519.4787640571594, 517.0326201915741, 514.6028609275818, 512.1883540153503, 509.78805589675903, 507.4008240699768, 505.0261323451996, 502.6616702079773, 500.30630111694336, 497.9597191810608, 495.6220886707306, 493.29232954978943, 490.9706451892853, 488.6558322906494, 486.3483316898346, 484.0473816394806, 481.752153635025, 479.4602644443512, 477.1714360713959, 474.8845126628876, 472.59929394721985, 470.31683015823364, 468.0363202095032, 465.75568532943726, 463.47450947761536, 461.19114685058594, 458.9066233634949, 456.6196632385254, 454.33201575279236, 452.04023790359497, 449.7452201843262, 447.44635939598083, 445.14172887802124, 442.83183240890503, 440.5163514614105, 438.19497656822205, 435.86787128448486, 433.53356409072876, 431.1936423778534, 428.8464186191559, 426.49256896972656, 424.13024139404297, 421.7598783969879, 419.3804044723511, 416.9916594028473, 414.5930941104889, 412.18665277957916, 409.7707533836365, 407.34636294841766, 404.91226387023926, 402.4693182706833, 400.01658618450165, 397.5539492368698, 395.0807534456253, 392.59535670280457, 390.1008154153824, 387.5970251560211, 385.0818898677826, 382.55391561985016, 380.0147706270218, 377.4674632549286, 374.9101915359497, 372.34105038642883, 369.762198805809, 367.173575758934, 364.5762904882431, 361.96681547164917, 359.35011196136475, 356.72184216976166, 354.08459746837616, 351.4392429590225, 348.78455328941345, 346.12223863601685, 343.45123839378357, 340.7749094963074, 338.091188788414, 335.4009653329849, 332.70421850681305, 329.9999653697014, 327.2918743491173, 324.57582956552505, 321.85689574480057, 319.1336193680763, 316.40919095277786, 313.6793424487114, 310.9490582346916, 308.21650356054306, 305.4817002415657, 302.74662178754807, 300.0094164609909, 297.26887744665146, 294.5289563536644, 291.7886029481888, 289.0463835000992, 286.3081300854683, 283.56874549388885, 280.8322105407715, 278.0958543419838, 275.3620240688324, 272.63103729486465, 269.9024980068207, 267.17965865135193, 264.4615302681923, 261.74603593349457, 259.0358347296715, 256.3341272473335, 253.63558876514435, 250.94576960802078, 248.26254552602768, 245.5860134959221, 242.91903203725815, 240.25940877199173, 237.6086676120758, 234.96636724472046, 232.33722645044327, 229.71549052000046, 227.10480105876923, 224.50477474927902, 221.91673630475998, 219.3394439816475, 216.77670001983643, 214.22449207305908, 211.68437093496323, 209.159550011158, 206.64892894029617, 204.1530860066414, 201.67093634605408, 199.20481729507446, 196.75347638130188, 194.31741580367088, 191.89726474881172, 189.4949769973755, 187.1078501343727, 184.73994120955467, 182.38657647371292, 180.05215719342232, 177.7344772517681, 175.43395745754242, 173.15223065018654, 170.88852295279503, 168.64329281449318, 166.4172421991825, 164.21085691452026, 162.02400705218315, 159.85545700788498, 157.70866507291794, 155.58083081245422, 153.47438955307007, 151.38693395256996, 149.32041600346565, 147.27343234419823, 145.24757453799248, 143.2428873181343, 141.2589092552662, 139.29652771353722, 137.35471057891846, 135.43666470050812, 133.539177775383, 131.66279006004333, 129.8085930943489, 127.97576206922531, 126.16510057449341, 124.3751939535141, 122.60922929644585, 120.86249449849129, 119.1398132443428, 117.43723672628403, 115.75724989175797, 114.09932863712311, 112.46361038088799, 110.84831437468529, 109.25620210170746, 107.68492230772972, 106.13494744896889, 104.60732445120811, 103.1000804901123, 101.61414921283722, 100.15040853619576, 98.70648962259293, 97.28426519036293, 95.88231000304222, 94.50221098959446, 93.14141930639744, 91.80302223563194, 90.48295992612839, 89.18423011898994, 87.90478014945984, 86.64532804489136, 85.40567722916603, 84.18576422333717, 82.98504094779491, 81.80278508365154, 80.64033336937428, 79.49625374376774, 78.37131994962692, 77.26426512002945, 76.17596942186356, 75.10535787045956, 74.05272725224495, 73.01693250238895, 71.99970655143261, 70.99926854670048, 70.01511316001415, 69.04831935465336, 68.09780879318714, 67.16360884904861, 66.24595829844475, 65.34401273727417, 64.45757161080837, 63.58650332689285, 62.730823293328285, 61.88942037522793, 61.06356206536293, 60.251839220523834, 59.45499400794506, 58.6716974824667, 57.90244872868061, 57.14701862633228, 56.405434623360634, 55.67602948844433, 54.960593178868294, 54.25780212879181, 53.567219853401184, 52.88975812494755, 52.223815739154816, 51.57058846950531, 50.92846319079399, 50.29829514026642, 49.67930580675602, 49.07119885087013, 48.47459453344345, 47.88778629899025, 47.31223700940609, 46.74660040438175, 46.19118717312813, 45.64540360122919, 45.10942529886961, 44.58324292302132, 44.06596182286739, 43.55838705599308, 43.05932819098234, 42.56926526129246, 42.08778867870569, 41.614606358110905, 41.15002840012312, 40.69372101128101, 40.24514254182577, 39.80467798560858, 39.37150455266237, 38.9463220462203, 38.52821038663387, 38.117438428103924, 37.71408589929342, 37.31720062345266, 36.92724855989218, 36.54416685551405, 36.167438082396984, 35.79732894152403, 35.43355777859688, 35.07543873041868, 34.7240746319294, 34.37834820896387, 34.03835836797953, 33.70392565429211, 33.3754720762372, 33.051829278469086, 32.73415780067444, 32.42122217267752, 32.11373146623373, 31.81121215224266, 31.513462401926517, 31.220755964517593, 30.932951666414738, 30.649514615535736, 30.370586901903152, 30.09624331444502, 29.826097272336483, 29.56033019721508, 29.298773869872093, 29.041064493358135, 28.787831895053387, 28.538103349506855, 28.2925448641181, 28.050329841673374, 27.812510289251804, 27.577855221927166, 27.346787586808205, 27.11921939253807, 26.895476259291172, 26.674460031092167, 26.457117177546024, 26.24296221882105, 26.031993709504604, 25.824241816997528, 25.619217358529568, 25.417462691664696, 25.21854618936777, 25.02277010679245, 24.82937853783369, 24.63907242566347, 24.45139030367136, 24.26635855436325, 24.083911679685116, 23.90411566197872, 23.726847797632217, 23.551997289061546, 23.379665695130825, 23.209653303027153, 23.042090870440006, 22.87673917412758, 22.713581435382366, 22.552675556391478, 22.3940384760499, 22.237338107079268, 22.08293490111828, 21.930541343986988, 21.78023948892951, 21.6317635551095, 21.485379580408335, 21.340785294771194, 21.198225520551205, 21.05733584985137, 20.91835442557931, 20.781174894422293, 20.64576482400298, 20.512060053646564, 20.379983592778444, 20.249688357114792, 20.121008399873972, 19.99398774653673, 19.868403896689415, 19.74446177110076, 19.622100964188576, 19.501130364835262, 19.381668847054243, 19.263810254633427, 19.147098891437054, 19.03214941173792, 18.918297700583935, 18.805902741849422, 18.694920428097248, 18.58515540137887, 18.476830020546913, 18.369636222720146, 18.263780642300844, 18.159055430442095, 18.05575505271554, 17.953576628118753, 17.85238979756832, 17.75264113396406, 17.653937835246325, 17.556355617940426, 17.45976998656988, 17.36443569138646, 17.270137168467045, 17.176792833954096, 17.084523651748896, 16.993443701416254, 16.903208933770657, 16.813942030072212, 16.72582409158349, 16.63849087432027, 16.552299574017525, 16.466768372803926, 16.3824249394238, 16.298869639635086, 16.216161105781794, 16.134519461542368, 16.05348263308406, 15.97347467765212, 15.894335586577654, 15.81579738855362, 15.738413628190756, 15.661539118736982, 15.585506435483694, 15.510419804602861, 15.43604838103056, 15.362339492887259, 15.289519224315882, 15.217394731938839, 15.145856194198132, 15.075213938951492, 15.005177021026611, 14.936106007546186, 14.867350809276104, 14.799434129148722, 14.732339397072792, 14.665607690811157, 14.599717367440462, 14.534543093293905, 14.469793647527695, 14.405826289206743, 14.342415940016508, 14.279661748558283, 14.217414747923613, 14.155859593302011, 14.094794873148203, 14.034389797598124, 13.974540933966637, 13.915196865797043, 13.856519356369972, 13.798271957784891, 13.740582335740328, 13.683519758284092, 13.626934826374054, 13.570853035897017, 13.515396531671286, 13.460252709686756, 13.405739467591047, 13.351722825318575, 13.298173006623983, 13.245074812322855, 13.192479208111763, 13.140308812260628, 13.088726740330458, 13.03752676025033, 12.986665770411491, 12.936410620808601, 12.88659393787384, 12.837071150541306, 12.78807794675231, 12.739505045115948, 12.69141822308302, 12.64358313754201, 12.596253387629986, 12.549309242516756, 12.502836167812347, 12.456645298749208, 12.410986263304949, 12.365576405078173, 12.320608768612146, 12.275994207710028, 12.231836628168821, 12.18797181546688, 12.144462648779154, 12.101427718997002, 12.058597169816494, 12.016237685456872, 11.974076483398676, 11.932410228997469, 11.891052102670074, 11.849919663742185, 11.809279806911945, 11.768883539363742, 11.728815322741866, 11.689037904143333, 11.649670261889696, 11.61051189340651, 11.571686362847686, 11.533254239708185, 11.495028857141733, 11.457112483680248, 11.41951286047697, 11.382238861173391, 11.345185119658709, 11.308458503335714, 11.272015130147338, 11.235842499881983, 11.19993919506669, 11.164352899417281, 11.12900727801025, 11.093938793987036, 11.059115482494235, 11.024552932009101, 10.99034876190126, 10.956279929727316, 10.922460282221437, 10.889049744233489, 10.855697523802519, 10.822713673114777, 10.789938746020198, 10.757425447925925, 10.725118367001414, 10.69310662895441, 10.661222238093615, 10.629644598811865, 10.598329285159707, 10.567166579887271, 10.536289874464273, 10.505567038431764, 10.475148944184184, 10.44488257728517, 10.414870262145996, 10.384978417307138, 10.3554912712425, 10.326035790145397, 10.29682139120996, 10.267911344766617, 10.23906290344894, 10.210551278665662, 10.182096801698208, 10.15399525500834, 10.125960428267717, 10.098205974325538, 10.070567352697253, 10.043159753084183, 10.015945261344314, 9.988923164084554, 9.962093938142061, 9.9353356808424, 9.908915581181645, 9.882587756961584, 9.856433402746916, 9.830497922375798, 9.804684195667505, 9.77913998439908, 9.753658972680569, 9.728308649733663, 9.703322026878595, 9.678337406367064, 9.653569703921676, 9.628928747028112, 9.604555564001203, 9.580159220844507, 9.556079963222146, 9.53209719993174, 9.50823530741036, 9.484637584537268, 9.461072882637382, 9.43771612457931, 9.414499567821622, 9.391436466947198, 9.368512976914644, 9.345743115991354, 9.32308672182262, 9.300643634051085, 9.27826453000307, 9.256018102169037, 9.234031254425645, 9.212041612714529, 9.190295860171318, 9.16858652047813, 9.147119520232081, 9.125684384256601, 9.104452561587095, 9.08328285254538, 9.062273178249598, 9.041474619880319, 9.020643951371312, 9.000089881941676, 8.979561015963554, 8.9592180326581, 8.938969511538744, 8.918865982443094, 8.89881850220263, 8.878946525976062, 8.859221115708351, 8.839545210823417, 8.820023532956839, 8.80059807933867, 8.78133424371481, 8.762142542749643, 8.743072709068656, 8.724099107086658, 8.705318791791797, 8.686511225998402, 8.66789915971458, 8.649398144334555, 8.630985610187054, 8.61269124597311, 8.594447871670127, 8.576374610885978, 8.558433273807168, 8.540487727150321, 8.522753670811653, 8.505050921812654, 8.487485270947218, 8.46994604356587, 8.452594235539436, 8.435341427102685, 8.418109133839607, 8.401083014905453, 8.384027991443872, 8.367173448204994, 8.350336905568838, 8.333645520731807, 8.31697435118258, 8.300469452515244, 8.28406803868711, 8.2676955871284, 8.251474360004067, 8.235256830230355, 8.219265991821885, 8.203181566670537, 8.187336487695575, 8.171496748924255, 8.155775355175138, 8.140117432922125, 8.124538948759437, 8.109109438955784, 8.093681778758764, 8.078432202339172, 8.06313974969089, 8.048039419576526, 8.032934177666903, 8.017976703122258, 8.003045758232474, 7.98820860311389, 7.973488429561257, 7.9587774109095335, 7.944245321676135, 7.929656036198139, 7.915272546932101, 7.900867188349366, 7.88659418001771, 7.872356371954083, 7.8582394029945135, 7.844153536483645, 7.830136142671108, 7.816243261098862, 7.802363680675626, 7.788599608466029, 7.774850491434336, 7.761264458298683, 7.747649559751153, 7.734162997454405, 7.720697598531842, 7.707335522398353, 7.694087261334062, 7.6805887166410685, 7.667487058788538, 7.654312921687961, 7.641328003257513, 7.628295602276921, 7.615407891571522, 7.602626994252205, 7.589774567633867, 7.5771206598728895, 7.564438682049513, 7.551861820742488, 7.539317527785897, 7.526872171089053, 7.514463348314166, 7.502106159925461, 7.489864880219102, 7.477572061121464, 7.465493919327855, 7.453305015340447, 7.441324425861239, 7.429284658282995, 7.4173479732126, 7.405526911839843, 7.393643575720489, 7.381928981281817, 7.370192159898579, 7.358585086651146, 7.34692439250648, 7.335448667407036, 7.3238933477550745, 7.312524499371648, 7.301084988750517, 7.2897705575451255, 7.278519659303129, 7.267308627255261, 7.256159033626318, 7.244986457750201, 7.23398180026561, 7.2229130659252405, 7.212006043642759, 7.201064260676503, 7.190201735123992, 7.179444698616862, 7.168647484853864, 7.157945952378213, 7.147277953103185, 7.136686551384628, 7.126078394241631, 7.115627838298678, 7.1051102401688695, 7.094730610959232, 7.084348645992577, 7.073973502032459, 7.063774473965168, 7.053495558910072, 7.043327915482223, 7.0331834983080626, 7.023110527545214, 7.0130262691527605, 7.003075059503317, 6.993070362135768, 6.983136345632374, 6.973296113312244, 6.9634591024369, 6.9536911472678185, 6.943925159983337, 6.93427437543869, 6.924578999169171, 6.914994213730097, 6.905403012409806, 6.895908405072987, 6.886383889243007, 6.876920704729855, 6.867581135593355, 6.858189099468291, 6.848823384381831, 6.839637201279402, 6.83032994158566, 6.821151023730636, 6.811978527344763, 6.8028668705374, 6.793744472786784, 6.784771683625877, 6.77571177855134, 6.766785274259746, 6.757817228324711, 6.748949245549738, 6.740079679526389, 6.731241633184254, 6.72251651994884, 6.713740435428917, 6.705079861916602, 6.696365161798894, 6.687781808897853, 6.679120852611959, 6.670584878884256, 6.662093590013683, 6.653547149151564, 6.645161574706435, 6.636703889816999, 6.628356414847076, 6.619975185021758, 6.611696655862033, 6.60336178727448, 6.5951746460050344, 6.586919851601124, 6.578733925707638, 6.570629620924592, 6.56251232791692, 6.5544276321306825, 6.5463676350191236, 6.538381649181247, 6.5303550669923425, 6.522452296689153, 6.514506879262626, 6.506607568822801, 6.498825519345701, 6.490924783051014, 6.483194501139224, 6.475395824760199, 6.4676935924217105, 6.459972053766251, 6.4523464208468795, 6.4446612214669585, 6.437119068577886, 6.429478311911225, 6.4219358852133155, 6.4144835351035, 6.40692916046828, 6.399524653330445, 6.392057824879885, 6.384698759764433, 6.377293275669217, 6.369989335536957, 6.362635553814471, 6.355355775915086, 6.348135609179735, 6.340864459052682, 6.333715862594545, 6.3265037797391415, 6.319404670968652, 6.312235384248197, 6.305194055661559, 6.298114528879523, 6.291115767322481, 6.284073538146913, 6.277086938731372, 6.270196057856083, 6.263212968595326, 6.256357719190419, 6.2494516391307116, 6.242639250122011, 6.2357789017260075, 6.229030673392117, 6.222221881151199, 6.215466598980129, 6.208770298399031, 6.202060878276825, 6.195417835377157, 6.188736528158188, 6.182170356623828, 6.175534320995212, 6.169018907472491, 6.162435523234308, 6.155934752896428, 6.14941034372896, 6.1429792465642095, 6.136477124877274, 6.130120030604303, 6.123664463870227, 6.11733315512538, 6.110938725061715, 6.104601473547518, 6.098336186259985, 6.092029761523008, 6.085803622379899, 6.07951292861253, 6.073360654525459, 6.067114788107574, 6.060985481366515, 6.054804120212793, 6.0486893923953176, 6.042572748847306, 6.036523134447634, 6.030428435653448, 6.024406637065113, 6.018421059474349, 6.012370428070426, 6.00644694454968, 6.000457825139165, 5.994562919251621, 5.9886154076084495, 5.98277906794101, 5.976862465962768, 5.97104459349066, 5.965187874622643, 5.959411793388426, 5.953600284643471, 5.947852751240134, 5.942108627408743, 5.936357752420008, 5.930693087168038, 5.924968158826232, 5.919341719709337, 5.91365166939795, 5.908071684651077, 5.902437916025519, 5.8968904903158545, 5.891271106898785, 5.885761650279164, 5.880200476385653, 5.8747254172340035, 5.869193399325013, 5.863776213489473, 5.858264875598252, 5.852877436205745, 5.847426467575133, 5.842002990655601, 5.8366942051798105, 5.831281546503305, 5.825992800295353, 5.820608372800052, 5.815358831547201, 5.810016541741788, 5.804790578782558, 5.799489125609398, 5.794315066188574, 5.78904601931572, 5.783849664963782, 5.778701406903565, 5.773522959090769, 5.76837604958564, 5.763235251419246, 5.758145703934133, 5.753002221696079, 5.747990315780044, 5.742858497425914, 5.737866437993944, 5.732796801254153, 5.727813531644642, 5.722781230695546, 5.71785185392946, 5.712849546223879, 5.707932864315808, 5.702966648153961, 5.698048301041126, 5.693190109916031, 5.688271079212427, 5.683445264585316, 5.678581897169352, 5.673758569173515, 5.668907990679145, 5.664138634689152, 5.659313315525651, 5.6545866979286075, 5.6497853845357895, 5.645107471384108, 5.6403318559750915, 5.635641658678651, 5.6309207594022155, 5.626294728368521, 5.6215709103271365, 5.616981418803334, 5.612290827557445, 5.607663271017373, 5.603105545975268, 5.598466446623206, 5.593935525976121, 5.5893449150025845, 5.584834038279951, 5.580257176421583, 5.575775424018502, 5.571235708892345, 5.566779835149646, 5.562259924598038, 5.557861444540322, 5.553359728306532, 5.548947382718325, 5.544498671777546, 5.540134388953447, 5.535704859532416, 5.531373440288007, 5.5269581861793995, 5.522653193213046, 5.51826492138207, 5.513937679119408, 5.509679063223302, 5.505326162092388, 5.501097955740988, 5.496780456975102, 5.492551515810192, 5.4882717141881585, 5.484082609415054, 5.479814022779465, 5.4756725979968905, 5.47141775675118, 5.467278459109366, 5.463075402192771, 5.458958229981363, 5.454774260520935, 5.450659066438675, 5.446557399816811, 5.442424233071506, 5.438365085981786, 5.434244860894978, 5.4302299078553915, 5.426147795282304, 5.422119211405516, 5.418054481036961, 5.41405750438571, 5.410026188008487, 5.40605759061873, 5.402046448551118, 5.398124011233449, 5.394100575707853, 5.39020103123039, 5.3862261744216084, 5.382297454401851, 5.3783892104402184, 5.374490497633815, 5.370592238381505, 5.366737652570009, 5.362861672416329, 5.358991948887706, 5.355148762464523, 5.35132950078696, 5.3474950501695275, 5.343670226633549, 5.3398895598948, 5.336095971055329, 5.332305525429547, 5.328605089336634, 5.324788611382246, 5.321048892103136, 5.317313661798835, 5.313584953546524, 5.309880374930799, 5.306173932738602, 5.302500787191093, 5.298809804953635, 5.295135289430618, 5.291476683691144, 5.287825237028301, 5.284180390648544, 5.280572529882193, 5.276947565376759, 5.273327641189098, 5.269782733172178, 5.266153975389898, 5.26256762444973, 5.25906446762383, 5.255460642278194, 5.251946757547557, 5.24837963655591, 5.2448920318856835, 5.241335575468838, 5.237880987115204, 5.2343253288418055, 5.230895602144301, 5.2273662043735385, 5.223943301476538, 5.220445433631539, 5.217012358829379, 5.213593638502061, 5.210133443586528, 5.2067157831043005, 5.203315882012248, 5.199920177459717, 5.196533355861902, 5.193142237141728, 5.18975995760411, 5.186412221752107, 5.183050766587257, 5.179699371568859, 5.17639363463968, 5.173044538125396, 5.169727576896548, 5.166466570459306, 5.163124348968267, 5.159877169877291, 5.156571418046951]\n",
      "tensor([-0.6540,  0.6477,  1.3942,  1.3894, -0.5976,  1.2295, -0.0934, -0.6947,\n",
      "         1.5009, -0.5381], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size) -> None:\n",
    "        super(NGramLanguageModeler,self).__init__() \n",
    "        self.embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        embeds = self.embeddings(inputs).view((1,-1)) # adding a second dim to vector\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out,dim=1)\n",
    "        return log_probs\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab),EMBEDDING_DIM,CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    total_loss =0\n",
    "    for context, target in ngrams:\n",
    "        \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        \n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "        \n",
    "        #step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        \n",
    "        #step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "print(losses) # The loss decreased every iteration over the training data!\n",
    "\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "print(model.embeddings.weight[word_to_ix['beauty']])\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyBulletRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4808ead1ed8c365cb0372690296c96d5824e3eeec0ef8755e9d8773c326899f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
